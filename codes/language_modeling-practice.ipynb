{"cells":[{"cell_type":"markdown","metadata":{"id":"-uXkcYhkIxS-"},"source":["#  <font color='#FFE15B'><b> Language Modeling </b></font>"]},{"cell_type":"markdown","metadata":{"id":"w_a3OXnSeV0z"},"source":["# üî¥ **Import Libs**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9IIS150p49GS"},"outputs":[],"source":["!pip install torchtext==0.15.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BNj9eLQoF1BP"},"outputs":[],"source":["!pip install -q torchdata==0.4.1"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13528,"status":"ok","timestamp":1713181338350,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"vOkiMO-v90zt","outputId":"5b6a039f-aea8-4749-8372-4c92188f0298"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/841.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m225.3/841.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m655.4/841.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q torchmetrics"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":11745,"status":"ok","timestamp":1713181324834,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"uxEgGvjYPFpl"},"outputs":[],"source":["!pip install -q portalocker>=2.0.0"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9572,"status":"ok","timestamp":1713181977970,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"vhlVJEkJeTsV"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torchtext\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader, Dataset\n","\n","from torch import optim\n","from torch.nn import functional as F\n","\n","import tqdm\n","import torchmetrics as tm"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1713181356357,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"DEzYlyeqTZqQ","outputId":"5854649e-d84e-4579-bc1b-9248092d5d27"},"outputs":[{"name":"stdout","output_type":"stream","text":["Python 3.10.12\n","2.0.1+cu117\n","0.15.2+cpu\n"]}],"source":["!python --version\n","print(torch.__version__)\n","print(torchtext.__version__)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1713181356948,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"6DWjGTq6T8Jg","outputId":"caab0d3c-2c08-484f-818e-886a3c586308"},"outputs":[{"name":"stdout","output_type":"stream","text":["numpy --> 1.25.2\n","torch --> 2.0.1+cu117\n","torchtext --> 0.15.2+cpu\n","tqdm --> 4.66.2\n"]}],"source":["for lib in [np, torch, torchtext, tqdm]:\n","  print(lib.__name__, '-->', lib.__version__)"]},{"cell_type":"markdown","metadata":{"id":"RwaY_YcgRayy"},"source":["# üî¥ **Utils**"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713181977970,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"8yMS7bbmRayz"},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1713181977970,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"PpKbTUEIRayz"},"outputs":[],"source":["def num_trainable_params(model):\n","  nums = sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n","  return nums"]},{"cell_type":"markdown","metadata":{"id":"RTql4Ftiunfr"},"source":["# üî¥ **Dataset**"]},{"cell_type":"markdown","metadata":{"id":"ujIVtjsYvxOI"},"source":["## üü† **Load the Dataset**"]},{"cell_type":"markdown","metadata":{"id":"Ek9DpCNCChzF"},"source":["üî∞ In this session you should load WikiText2 dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tkgTXXz3xORk"},"outputs":[],"source":["# n * b * l >> no need to dataloader\n","# n * l     >> need dataloader"]},{"cell_type":"code","execution_count":88,"metadata":{"executionInfo":{"elapsed":451,"status":"ok","timestamp":1713187760187,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"ShYpXvVzVmP6"},"outputs":[],"source":["from torchtext.datasets import WikiText2\n","train_iter, valid_iter, test_iter = WikiText2(root='/content/datasets/WikiText2',\n","                                              split=('train', 'valid', 'test'))"]},{"cell_type":"code","execution_count":89,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":421,"status":"ok","timestamp":1713187764020,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"_3fACs75s_vt","outputId":"7a828d7c-6815-4dd9-dd64-a869d99ff182"},"outputs":[{"data":{"text/plain":["ShardingFilterIterDataPipe"]},"execution_count":89,"metadata":{},"output_type":"execute_result"}],"source":["train_iter"]},{"cell_type":"code","execution_count":117,"metadata":{"executionInfo":{"elapsed":535,"status":"ok","timestamp":1713191578348,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"oWu_99lM_86y"},"outputs":[],"source":["class CustomDataset(torch.utils.data.IterableDataset):\n","\n","  def __init__(self, file_path):\n","    self.file_path = file_path\n","\n","  def parse_file(self):\n","    with open(self.file_path, 'r') as file:\n","      for line in file:\n","        yield line.strip()\n","\n","  def __iter__(self):\n","    return self.parse_file()"]},{"cell_type":"code","execution_count":118,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1713191580358,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"RAcBbDRPGTDU"},"outputs":[],"source":["train_dataset = CustomDataset('/content/datasets/WikiText2/wiki.train.tokens')\n","valid_dataset = CustomDataset('/content/datasets/WikiText2/wiki.valid.tokens')\n","test_dataset  = CustomDataset('/content/datasets/WikiText2/wiki.test.tokens')"]},{"cell_type":"code","execution_count":119,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"elapsed":677,"status":"error","timestamp":1713191582560,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"-4BTM1zgGiQT","outputId":"25ab98bc-a0c7-4679-f320-098b40ba4184"},"outputs":[{"ename":"TypeError","evalue":"'CustomDataset' object is not an iterator","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-119-ef34d1132b68>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: 'CustomDataset' object is not an iterator"]}],"source":["next(train_dataset)"]},{"cell_type":"code","execution_count":90,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":506,"status":"ok","timestamp":1713187767264,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"taG8IdgXwe_y","outputId":"564d0fda-0a11-4038-855c-ad7fccf2172a"},"outputs":[{"data":{"text/plain":["<generator object ShardingFilterIterDataPipe.__iter__ at 0x7b566d9dd0e0>"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["train_iter = iter(valid_iter)\n","train_iter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"spiKwNY8-MhW"},"outputs":[],"source":["next(train_iter)"]},{"cell_type":"markdown","metadata":{"id":"wCi-ofSLCzop"},"source":["## üü† **Build vocabulary and save it**"]},{"cell_type":"markdown","metadata":{"id":"L02PHFuyNRb3"},"source":["üî∞ In this section we need to:\n","\n","*   Define a tokenizer using `basic_english`\n","*   Tokenize the dataset and collect tokens\n","*   Build the vocabulary using `build_vocab_from_iterator`\n","*   Manually insert special tokens and set the default index\n"]},{"cell_type":"code","execution_count":92,"metadata":{"executionInfo":{"elapsed":428,"status":"ok","timestamp":1713187777572,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"dlJ6Q6xCVuf0"},"outputs":[],"source":["from torchtext.data.utils import get_tokenizer\n","tokenizer = get_tokenizer(\"basic_english\")"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":432,"status":"ok","timestamp":1713187781516,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"kE2iu-K7vgB3","outputId":"ed90dadb-d7f1-4110-eca6-031f471631bf"},"outputs":[{"ename":"NameError","evalue":"name 'get_tokenizer' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m txt \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@sajad hi sajad! 1 n2 3 #45\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhow are are you?\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasic_english\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m [tokenizer(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m txt]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# list(map(tokenizer, txt))\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'get_tokenizer' is not defined"]}],"source":["txt = ['@sajad hi sajad! 1 n2 3 #45', 'how are are you?']\n","\n","tokenizer = get_tokenizer('basic_english')\n","\n","[tokenizer(line) for line in txt]\n","# list(map(tokenizer, txt))"]},{"cell_type":"code","execution_count":94,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":510,"status":"ok","timestamp":1713187785808,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"fM870-eCw7hj","outputId":"862256de-85f7-41f7-c09d-fe8cf0a5a924"},"outputs":[{"data":{"text/plain":["{'sajad': 11,\n"," 'n2': 10,\n"," 'how': 9,\n"," '@sajad': 7,\n"," '?': 6,\n"," '3': 5,\n"," '1': 4,\n"," '#45': 3,\n"," '!': 2,\n"," 'are': 1,\n"," 'you': 12,\n"," 'hi': 8,\n"," '<unk>': 0}"]},"execution_count":94,"metadata":{},"output_type":"execute_result"}],"source":["from torchtext.vocab import build_vocab_from_iterator\n","\n","vocab = build_vocab_from_iterator(map(tokenizer, txt), specials=['<unk>'], min_freq=1)\n","vocab.set_default_index(vocab['<unk>'])\n","vocab.get_stoi()"]},{"cell_type":"code","execution_count":95,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":459,"status":"ok","timestamp":1713187790539,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"ii-a2mZezAPE","outputId":"7da5d6e8-87f5-4e58-f4c4-a410df63fa8b"},"outputs":[{"data":{"text/plain":["[0, 8, 0]"]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["vocab(['ebi', 'hi', 'qwerty'])"]},{"cell_type":"code","execution_count":96,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713187792030,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"N_rizZEw8buw"},"outputs":[],"source":["tokenizer = get_tokenizer('basic_english')\n","vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n","vocab.set_default_index(vocab['<unk>'])"]},{"cell_type":"code","execution_count":97,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713187792030,"user":{"displayName":"Ebi mousavi","userId":"11904011802302855819"},"user_tz":-210},"id":"7feN-ArW9XM2","outputId":"92a38632-3bd3-462f-e01f-b99af0feb7c6"},"outputs":[{"data":{"text/plain":["1"]},"execution_count":97,"metadata":{},"output_type":"execute_result"}],"source":["len(vocab)"]},{"cell_type":"markdown","metadata":{"id":"B29jrEvwRqXA"},"source":["## üü† EDA"]},{"cell_type":"markdown","metadata":{"id":"pHtoYxEPd3bL"},"source":["### üü° Let's explore the WikiText2 dataset!"]},{"cell_type":"markdown","metadata":{"id":"A3rnR739GbYb"},"source":["### üü° Calculate basic statistics such as the number of documents, total words, average document length, etc."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHVKeKwk2WaG"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"a4HyLPqcsF43"},"source":["### üü° Analyze the most common and least common words in the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBnEjagdTN8n"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"cfBasjQCE_aI"},"source":["### üü°  Please proceed with further exploration of the dataset. what do you suggest?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yR8uQsv4E_aJ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"idRexFij4wgN"},"source":["## üü† Transform the data"]},{"cell_type":"markdown","metadata":{"id":"2VjvBOtvHu2v"},"source":["üõë Make sure to perform the transformations on train, validation and test datasets."]},{"cell_type":"markdown","metadata":{"id":"ApisIcGeGSsJ"},"source":["üî∞ Reshape the dataset into an `N x B x L` or `M x L` format, where `N` represents the number of batches, `B` is the batch size, `L` is the length of a sample within each batch, and `M` is equal to `N x B`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ocxM8YdsWH-1"},"outputs":[],"source":["def data_process(raw_text_iter, batch_size, seq_len):\n","\n","    return inputs, targets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4GndG2B0WPIb"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"PgLgP04P4-aX"},"source":["## üü† Custom dataset"]},{"cell_type":"markdown","metadata":{"id":"XkxH_IR2PBNq"},"source":["üî∞ Write a custom dataset class for LanguageModelDataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1cjpSkrtexap"},"outputs":[],"source":["class LanguageModelDataset(Dataset):\n","\n","  def __init__(self, inputs, targets):\n","    pass\n","\n","  def __len__(self):\n","    pass\n","\n","  def __getitem__(self, idx):\n","    pass\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o0qUkL0CfQmr"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"NCQjacybOfqV"},"source":["## üü† Define a dataloader if needed"]},{"cell_type":"markdown","metadata":{"id":"HqKMEyFNS-1a"},"source":["üî∞ Write dataloaders for the training, validation, and test sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KMCJ3UMD0U_f"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"3ttl0AK3Hvyh"},"source":["# üî¥ **Model**"]},{"cell_type":"markdown","metadata":{"id":"06p-oBowTf-R"},"source":["üî∞ Use the following template to create a custom model.\n","\n","Your model should consist of three parts:\n","\n","*   an embedding layer\n","*   an LSTM layer\n","*   a fully connected layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ISnnHE0BMVqp"},"outputs":[],"source":["class LanguageModel(nn.Module):\n","\n","  def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate):\n","    pass\n","\n","  def forward(self, src):\n","    pass\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2MgBVzorb9oQ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"24qT-sgUO2-d"},"source":["# üî¥ **Config**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ma28M5Z36gsq"},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"markdown","metadata":{"id":"bwYDJKjuduUT"},"source":["üî∞ Define the optimizer, loss function, metrics and other necessary parameters in this section, and ensure the model is sent to the appropriate device."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ubk3xKaIG6i"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"W0QNbC0YPCKZ"},"source":["# üî¥ **Train ‚û∞**"]},{"cell_type":"markdown","metadata":{"id":"yS6EF4HUhi5e"},"source":["üî∞ This is the template for train function, change it if needed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WniOAgk0QyRI"},"outputs":[],"source":["def train_one_epoch(model, train_loader, loss_fn, optimizer, metric, epoch=None):\n","  model.train()\n","  loss_train = AverageMeter()\n","  metric.reset()\n","\n","  with tqdm.tqdm(train_loader, unit='batch') as tepoch:\n","    for inputs, targets in tepoch:\n","      if epoch:\n","        tepoch.set_description(f'Epoch {epoch}')\n","\n","      inputs = inputs.to(device)\n","      targets = targets.to(device)\n","\n","      outputs = model(inputs)\n","\n","      loss = loss_fn(outputs, targets)\n","\n","      loss.backward()\n","\n","      optimizer.step()\n","      optimizer.zero_grad()\n","\n","      loss_train.update(loss.item(), n=len(targets))\n","      metric.update(outputs, targets)\n","\n","      tepoch.set_postfix(loss=loss_train.avg, metric=metric.compute().item())\n","\n","  return model, loss_train.avg, metric.compute().item()"]},{"cell_type":"markdown","metadata":{"id":"G9HgVWslPGsH"},"source":["# üî¥ **Evaluation**"]},{"cell_type":"markdown","metadata":{"id":"TsszJ7GVj2l3"},"source":["üî∞ This is the template for evaluation function, change it if needed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uV0_67_ZQ0xf"},"outputs":[],"source":["def evaluate(model, test_loader, loss_fn, metric):\n","  model.eval()\n","  loss_eval = AverageMeter()\n","  metric.reset()\n","\n","  with torch.inference_mode():\n","    for inputs, targets in test_loader:\n","      inputs = inputs.to(device)\n","      targets = targets.to(device)\n","\n","      outputs = model(inputs)\n","\n","      loss = loss_fn(outputs, targets)\n","      loss_eval.update(loss.item(), n=len(targets))\n","\n","      metric(outputs, targets)\n","\n","  return loss_eval.avg, metric.compute().item()"]},{"cell_type":"markdown","metadata":{"id":"o_5f69nwPtY2"},"source":["# üî¥ **Training Process „ÄΩÔ∏è**"]},{"cell_type":"markdown","metadata":{"id":"De7VreNxQdct"},"source":["## üü† Finding Hyper-parameters"]},{"cell_type":"markdown","metadata":{"id":"lpJ3wtyctQJH"},"source":["### üü° **Step 1:** Calculate the loss for an untrained model using a few batches.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QnE4F4GkzzaR"},"outputs":[],"source":["model =\n","\n","inputs, targets = next(iter(train_set))\n","inputs = inputs.to(device)\n","targets = targets.to(device)\n","\n","with torch.no_grad():\n","  outputs = model(inputs)\n","  loss = loss_fn(outputs, targets)\n","\n","print(loss)"]},{"cell_type":"markdown","metadata":{"id":"BrHQCv7q7LF_"},"source":["### üü° **Step 2:** Try to train and overfit the model on a small subset of the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G0ji0MXsWaPt"},"outputs":[],"source":["model =\n","optimizer = torch.optim.SGD(model.parameters(), lr=, momentum=0.9)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kPRZQpPWJ2qv"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bNrg4d9hWaPt"},"outputs":[],"source":["num_epochs = ...\n","for epoch in range(num_epochs):\n","  model, _, _ = train_one_epoch(model, ..., loss_fn, optimizer, metric, epoch)"]},{"cell_type":"markdown","metadata":{"id":"BLT4w0ZfAhlJ"},"source":["### üü° **Step 3:** Train the model for a limited number of epochs, experimenting with various learning rates."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jxz5DXoj61mg"},"outputs":[],"source":["num_epochs =\n","\n","for lr in [...]:\n","  print(f'LR={lr}')\n","\n","  model =\n","  optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=1e-4, momentum=0.9)\n","\n","  for epoch in range(num_epochs):\n","    model, _, _ = train_one_epoch(model, train_set, loss_fn, optimizer, metric, epoch)\n","\n","  print()"]},{"cell_type":"markdown","metadata":{"id":"uC2GhaXfA8vC"},"source":["### üü° Step 4: Create a small grid using the weight decay and the best learning rate.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7UeNW3WWaPu"},"outputs":[],"source":["num_epochs =\n","\n","for lr in [...]:\n","  for wd in [...]:\n","    print(f'LR={lr}, WD={wd}')\n","\n","    model =\n","    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd)\n","\n","    for epoch in range(num_epochs):\n","      model, loss, _ = train_one_epoch(model, train_loader, loss_fn, optimizer, epoch)\n","\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"Mjd9Z3N1ef3I"},"source":["### üü° Step 5: Train model for longer epochs using the best model from step 4.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IWgkMgC6JWpU"},"outputs":[],"source":["model ="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YVwLp-02JWpV"},"outputs":[],"source":["lr =\n","wd =\n","optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd, momentum=0.9, nesterov=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zqxSVVB7JWpW"},"outputs":[],"source":["loss_train_hist = []\n","loss_valid_hist = []\n","\n","metric_train_hist = []\n","metric_valid_hist = []\n","\n","best_loss_valid = torch.inf\n","epoch_counter = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVqS9SEPJWpW"},"outputs":[],"source":["num_epochs =\n","\n","for epoch in range(num_epochs):\n","  # Train\n","  model, loss_train, metric_train = train_one_epoch(model,\n","                                                 train_set,\n","                                                 loss_fn,\n","                                                 optimizer,\n","                                                 metric,\n","                                                 epoch)\n","  # Validation\n","  loss_valid, metric_valid = evaluate(model,\n","                                     valid_set,\n","                                     loss_fn,\n","                                     metric)\n","\n","  loss_train_hist.append(loss_train)\n","  loss_valid_hist.append(loss_valid)\n","\n","  metric_train_hist.append(metric_train)\n","  metric_valid_hist.append(metric_valid)\n","\n","  if loss_valid < best_loss_valid:\n","    torch.save(model, f'model.pt')\n","    best_loss_valid = loss_valid\n","    print('Model Saved!')\n","\n","  print(f'Valid: Loss = {loss_valid:.4}, Metric = {metric_valid:.4}')\n","  print()\n","\n","  epoch_counter += 1"]},{"cell_type":"markdown","metadata":{"id":"rjGQ-M02cusP"},"source":["## üü† Main Loop"]},{"cell_type":"markdown","metadata":{"id":"4AdYaMU4x34g"},"source":["üî∞ Define model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JCtZXDybxexf"},"outputs":[],"source":["model ="]},{"cell_type":"markdown","metadata":{"id":"AUKZRiQPxqrB"},"source":["üî∞ Define optimizer and Set learning rate and weight decay."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bowjVB5yIXUP"},"outputs":[],"source":["lr =\n","wd =\n","optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd, momentum=0.9, nesterov=True)"]},{"cell_type":"markdown","metadata":{"id":"AUyFFIzlyaiB"},"source":["üî∞ Write code to train the model for `num_epochs` epoches."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CAXagB4yvtZd"},"outputs":[],"source":["loss_train_hist = []\n","loss_valid_hist = []\n","\n","metric_train_hist = []\n","metric_valid_hist = []\n","\n","best_loss_valid = torch.inf\n","epoch_counter = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PovABWnU3ld0"},"outputs":[],"source":["num_epochs =\n","\n","for epoch in range(num_epochs):\n","  # Train\n","  model, loss_train, metric_train = train_one_epoch(model,\n","                                                 train_set,\n","                                                 loss_fn,\n","                                                 optimizer,\n","                                                 metric,\n","                                                 epoch)\n","  # Validation\n","  loss_valid, metric_valid = evaluate(model,\n","                                     valid_set,\n","                                     loss_fn,\n","                                     metric)\n","\n","  loss_train_hist.append(loss_train)\n","  loss_valid_hist.append(loss_valid)\n","\n","  metric_train_hist.append(metric_train)\n","  metric_valid_hist.append(metric_valid)\n","\n","  if loss_valid < best_loss_valid:\n","    torch.save(model, f'model.pt')\n","    best_loss_valid = loss_valid\n","    print('Model Saved!')\n","\n","  print(f'Valid: Loss = {loss_valid:.4}, Metric = {metric_valid:.4}')\n","  print()\n","\n","  epoch_counter += 1"]},{"cell_type":"markdown","metadata":{"id":"oK20iNRI3Xxb"},"source":["## üü† Plot"]},{"cell_type":"markdown","metadata":{"id":"IKlLvCwuzEAA"},"source":["üî∞ Plot learning curves"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYFzTsdIOkVp"},"outputs":[],"source":["plt.figure(figsize=(8, 6))\n","\n","plt.plot(range(epoch_counter), loss_train_hist, 'r-', label='Train')\n","plt.plot(range(epoch_counter), loss_valid_hist, 'b-', label='Validation')\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('loss')\n","plt.grid(True)\n","plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"KZ9UIdmkfxlA"},"source":["# üî¥ **Test**"]},{"cell_type":"markdown","metadata":{"id":"SO8iPWH1zVYn"},"source":["üî∞ Test your model using data from the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"35sn67IhKcm_"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"FzcQQwFuar_7"},"source":["# üî¥ **Generate**"]},{"cell_type":"markdown","metadata":{"id":"jh2_9jUp0GF4"},"source":["üî∞ Your mission is to write a `generate` function and use a desired sentence to evaluate the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pskvb--R-wJ0"},"outputs":[],"source":["model_path = 'model.pt'\n","model = torch.load(model_path)\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5SvSDLal8YB"},"outputs":[],"source":["def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, seed=None):\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pVedneOVD6ul"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["w_a3OXnSeV0z","RwaY_YcgRayy","RTql4Ftiunfr","ujIVtjsYvxOI","wCi-ofSLCzop","B29jrEvwRqXA","A3rnR739GbYb","a4HyLPqcsF43","cfBasjQCE_aI","idRexFij4wgN","PgLgP04P4-aX","NCQjacybOfqV","3ttl0AK3Hvyh","24qT-sgUO2-d","W0QNbC0YPCKZ","G9HgVWslPGsH","o_5f69nwPtY2","De7VreNxQdct","lpJ3wtyctQJH","BrHQCv7q7LF_","BLT4w0ZfAhlJ","uC2GhaXfA8vC","Mjd9Z3N1ef3I","rjGQ-M02cusP","oK20iNRI3Xxb","KZ9UIdmkfxlA","FzcQQwFuar_7"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
